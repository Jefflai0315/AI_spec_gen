{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import result\n",
    "from attr import attr\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name = []\n",
    "prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open new soup with a new url\n",
    "def get_page(url):\n",
    "    response =  requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('server responded: ', response.status_code)\n",
    "    else:\n",
    "        data=response.text\n",
    "        soup=BeautifulSoup(data,\"html.parser\")\n",
    "        return soup\n",
    "\n",
    "# use in the individual product page to find details\n",
    "def get_detail_data(soup):\n",
    "    try:\n",
    "        title = soup.find('h1', attrs = {'class':\"x-item-title__mainTitle\"}).find('span').text\n",
    "    except:\n",
    "        title =' '\n",
    "\n",
    "    try:\n",
    "        price = soup.find('div', attrs = {'class':\"mainPrice\"}).find('span')\n",
    "        price = price.get('content')\n",
    "    except:\n",
    "        price =' '\n",
    "\n",
    "    try: \n",
    "        sold = soup.find('div', attrs = {'id':\"why2buy\"}).find('span').text #.split(' ')[0]\n",
    "    except:\n",
    "        sold =' '\n",
    "\n",
    "    try: \n",
    "        review = soup.find('div', attrs = {'class':\"overlay-top\"}).find('a') #.split(' ')[0]\n",
    "        reviewURL = review.get(\"href\") + '&pgn='\n",
    "        review_num = int(re.search(r'\\d+', review.text).group())\n",
    "        page = int(math.ceil(review_num/10))\n",
    "        print(reviewURL,page)\n",
    "    \n",
    "        review = get_review(reviewURL, page)\n",
    "        \n",
    "    except:\n",
    "        review =' '\n",
    "\n",
    "    try:\n",
    "        descs = soup.find('div', attrs ={'data-testid':\"ux-layout-section__item\",'class':\"ux-layout-section__item ux-layout-section__item--table-view\"}).find_all('span', attrs={'class':\"ux-textspans\"})\n",
    "    except:\n",
    "        descs = [' ']\n",
    "    finally:\n",
    "        specs = []\n",
    "        \n",
    "        for i in range(len(descs)):\n",
    "            descs = [i for i in descs if i.contents[1] !='Read more'] #remove read more expandable tag\n",
    "        \n",
    "\n",
    "        for i in range(len(descs)-1):\n",
    "            if i % 2 ==0:\n",
    "                \n",
    "                specs.append([descs[i].contents[1],descs[i+1].contents[1]])\n",
    "\n",
    "        if specs[0][1][:10] == specs[1][0][:10]:\n",
    "            specs[0][1] = specs[1][0]\n",
    "            del specs[1]\n",
    "        \n",
    "        print(review)\n",
    "    \n",
    "    return [title, price,sold,specs]\n",
    "\n",
    "\n",
    "# to see the specs of an individual product\n",
    "def get_single_df(specs):\n",
    "    df = pd.DataFrame(specs, columns = ['Specs title','Specs value'])\n",
    "    return df\n",
    "\n",
    "def get_review(url,page):\n",
    "    print(url,page)\n",
    "    reviews = []\n",
    "    url = url.split('urw')\n",
    "    # url1 = 'urw/Samsung-TU7000-43-4K-LED-Smart-TV-Titan-Gray'.join(url)\n",
    "    # print(url1)\n",
    "    print(\"in get_review\")\n",
    "    for i in range(page):\n",
    "        print(f'in loop{i}')\n",
    "        # url2 = url1 + str(0+i)\n",
    "        url = url + str(0+i)\n",
    "        soup = get_page(url)\n",
    "        print(soup)\n",
    "        \n",
    "\n",
    "        try: \n",
    "            review= soup.find_all('p',attrs= {'class':\"review-item-content rvw-wrap-spaces\", 'itemprop':\"reviewBody\"})\n",
    "        except:\n",
    "            review = ''\n",
    "        finally:\n",
    "            if review !=\"\":\n",
    "                reviews.extend(review)\n",
    "    \n",
    "    for i in range(len(reviews)):\n",
    "        reviews[i] = str(reviews[i]).replace('<p class=\"review-item-content rvw-wrap-spaces\" itemprop=\"reviewBody\">','')\n",
    "        reviews[i] = str(reviews[i]).replace('<span class=\"show-full-review\">','')\n",
    "        reviews[i] = str(reviews[i]).replace('</span>','')\n",
    "        reviews[i] = str(reviews[i]).replace('</br>','')\n",
    "        reviews[i] = str(reviews[i]).replace('<br/>','')\n",
    "        reviews[i] = str(reviews[i]).replace('<a class=\"show-full-review-link\" href=\"javascript:;\">Read full review...</a>','')\n",
    "\n",
    "    print(reviews)\n",
    "        \n",
    "    # strings = ','.join(reviews)\n",
    "    # print(strings)\n",
    "    # string1 = strings.replace('<p class=\"review-item-content rvw-wrap-spaces\" itemprop=\"reviewBody\">','')\n",
    "    \n",
    "    # try:\n",
    "    #     review_num = int(soupR.find('h2', attrs = {'class':\"reviews-section-title\"}).text.split(' ')[0])\n",
    "    # except:\n",
    "    #     review_num = 0\n",
    "    # finally:\n",
    "    #     page = math.ceil(review_num/10)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "https://www.ebay.com/urw/product-reviews/3037508876?_itm=154927478169&pgn= 4\n",
      "https://www.ebay.com/urw/product-reviews/3037508876?_itm=154927478169&pgn= 4\n",
      "in get_review\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "search_term = []\n",
    "# pass in search url of the search product page\n",
    "search_term.append('oven')\n",
    "ebayUrl = \"https://www.ebay.com/sch/i.html?_from=R40&_nkw=smart+TV&_sacat=0&LH_All=1&rt=nc&LH_ItemCondition=1000&_pgn=\"+str(1)\n",
    "r= requests.get(ebayUrl)\n",
    "data=r.text\n",
    "soup=BeautifulSoup(data,\"html.parser\")\n",
    "\n",
    "# get all listings \n",
    "listings = soup.find_all('li', attrs={'class': 's-item'})\n",
    "links = soup.find_all('a', class_ ='s-item__link')\n",
    "items = [item.get('href') for item in links] # store the link to each product \n",
    "\n",
    "array = []\n",
    "array_array = []\n",
    "for i in range(5):\n",
    "    soup =  get_page(items[i+1])   #open each product's url\n",
    "    results = get_detail_data(soup) \n",
    "    array.append(results)  #store all the details of a product\n",
    "array_array.append(array)\n",
    "\n",
    "# print(array_array)\n",
    "\n",
    "# return results = ['Object's title', 'Price', 'Sold', [['Specs title', 'Specs value']]]\n",
    "# return array  = [--list of results--]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e270d0dc8d2ed243472fe8a262aca5fef3dbd8bcd4d86377e8f2f1c935cb7ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
